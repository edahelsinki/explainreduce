{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ExplainReduce on the Auto MPG dataset\n",
    "In this notebook we will demonstrate how to use ExplainReduce on the Auto MPG dataset. We use Slisemap as the underlying local model generation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from slisemap import Slisemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model caching\n",
    "To reduce execution times, we provide dumps of pretrained Slisemap models. If you want to train them yourself instead please set USE_CACHE=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHE = False\n",
    "SM_CACHE_PATH = Path(\"cache\") / \"01_regression_example_autompg.sm\"\n",
    "\n",
    "if USE_CACHE:\n",
    "    for path in [SM_CACHE_PATH]:\n",
    "        path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        if not path.exists():\n",
    "            urlretrieve(\n",
    "                f\"https://raw.githubusercontent.com/edahelsinki/slisemap/data/examples/cache/{path.name}\",\n",
    "                path,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The Auto MPG dataset is a multivariate real-valued dataset with eight attributes describing fuel consumption -related properties of 398 distinct automobiles. We use mpg (miles per gallon) as the target variable and we have additionally removed 6 data items that had missing values. The data consists of 3 discrete and 5 continuous attributes (one of which is mpg) and it is available through UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Localadmin_mudonggu\\AppData\\Local\\Temp\\ipykernel_13772\\3030633475.py:16: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  auto_mpg = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the AutoMPG dataset (download it if necessary).\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].\n",
    " Irvine, CA: University of California, School of Information and Computer Science.\n",
    "Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning.\n",
    " In Proceedings on the Tenth International Conference of Machine Learning, 236-243,\n",
    " University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "\"\"\"\n",
    "path = Path(\"data\") / \"auto-mpg.data\"\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "if not path.exists():\n",
    "    urlretrieve(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\",\n",
    "        path,\n",
    "    )\n",
    "auto_mpg = pd.read_csv(\n",
    "    path,\n",
    "    names=[\n",
    "        \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\n",
    "        \"acceleration\", \"year\", \"origin\", \"carname\",\n",
    "    ],\n",
    "    delim_whitespace=True,\n",
    "    na_values=[\"?\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = auto_mpg[[\n",
    "    \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"year\", \"origin\",\n",
    "]]\n",
    "y0 = auto_mpg[\"mpg\"]\n",
    "\n",
    "# Split and one-hot encode the origin into USA vs Europe vs Japan\n",
    "X0 = np.concatenate(\n",
    "    (X0.values[:, :-1].astype(float), np.eye(3)[X0[\"origin\"].values.astype(int) - 1]), axis=1,\n",
    ")\n",
    "y0 = y0.values\n",
    "\n",
    "# X0 contains the covariates, y0 is the target variable and names are column names.\n",
    "mask = ~np.isnan(X0[:, 2])\n",
    "X0 = X0[mask]\n",
    "y0 = y0[mask]\n",
    "\n",
    "names = list(auto_mpg.columns[1:-2]) + [\"origin USA\", \"origin Europe\", \"origin Japan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin USA</th>\n",
       "      <th>origin Europe</th>\n",
       "      <th>origin Japan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0    18.0        8.0         307.0       130.0  3504.0          12.0  70.0   \n",
       "1    15.0        8.0         350.0       165.0  3693.0          11.5  70.0   \n",
       "2    18.0        8.0         318.0       150.0  3436.0          11.0  70.0   \n",
       "3    16.0        8.0         304.0       150.0  3433.0          12.0  70.0   \n",
       "4    17.0        8.0         302.0       140.0  3449.0          10.5  70.0   \n",
       "..    ...        ...           ...         ...     ...           ...   ...   \n",
       "387  27.0        4.0         140.0        86.0  2790.0          15.6  82.0   \n",
       "388  44.0        4.0          97.0        52.0  2130.0          24.6  82.0   \n",
       "389  32.0        4.0         135.0        84.0  2295.0          11.6  82.0   \n",
       "390  28.0        4.0         120.0        79.0  2625.0          18.6  82.0   \n",
       "391  31.0        4.0         119.0        82.0  2720.0          19.4  82.0   \n",
       "\n",
       "     origin USA  origin Europe  origin Japan  \n",
       "0           1.0            0.0           0.0  \n",
       "1           1.0            0.0           0.0  \n",
       "2           1.0            0.0           0.0  \n",
       "3           1.0            0.0           0.0  \n",
       "4           1.0            0.0           0.0  \n",
       "..          ...            ...           ...  \n",
       "387         1.0            0.0           0.0  \n",
       "388         0.0            1.0           0.0  \n",
       "389         1.0            0.0           0.0  \n",
       "390         1.0            0.0           0.0  \n",
       "391         1.0            0.0           0.0  \n",
       "\n",
       "[392 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.concatenate([y0.reshape((-1,1)), X0], axis=1), columns=[\"mpg\"]+names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y are normalised by `sklearn.preprocessing.StandardScaler`.\n",
    "scale_x = StandardScaler()\n",
    "scale_y = StandardScaler()\n",
    "X = np.concatenate([scale_x.fit_transform(X0[:, :-3]), X0[:, -3:]], axis=1)\n",
    "y = scale_y.fit_transform(y0[:, None])\n",
    "# We also remove ten random datapoints from the data for testing later.\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Black box model\n",
    "Instead of simply running Slisemap on the data itself, we showcase how it can be utilised to provide explanations on a black box model. Typically a black box model is produced via some machine learning algorithm and the inner workings of the resulting model are too complicated for a human to understand. Here we train and use a SVM from sklearn.svm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR().fit(X, y.ravel())\n",
    "y2 = svr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slisemap\n",
    "Use a Slisemap explainer to generate local models for the black-box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Localadmin_mudonggu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import explainreduce.localmodels as lm \n",
    "\n",
    "# Use a slisemap explainer\n",
    "sm_explainer = lm.SLISEMAPExplainer(\n",
    "    X=torch.tensor(X, dtype=torch.float32),\n",
    "    y=torch.tensor(y2, dtype=torch.float32),\n",
    "    lasso=0.01,\n",
    ")\n",
    "sm_explainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for each item in the training set (totally 385 items), we have a corresponding linear local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2509e-05,  7.0450e-06, -2.5707e-01,  ...,  5.4793e-06,\n",
       "           4.7097e-02, -7.6715e-06],\n",
       "         [-1.7261e-01, -1.1321e-01,  1.5296e-06,  ...,  1.3028e-05,\n",
       "           1.0497e-05, -7.4485e-06],\n",
       "         [ 3.9120e-06, -1.0666e-05, -2.5718e-01,  ...,  1.1157e-05,\n",
       "           4.6902e-02, -2.4783e-06],\n",
       "         ...,\n",
       "         [-1.7823e-01, -2.1140e-05, -2.5871e-05,  ..., -1.5368e-05,\n",
       "           9.4526e-07, -4.1667e-05],\n",
       "         [-2.3847e-05, -1.3585e-05, -2.5701e-01,  ...,  8.0615e-07,\n",
       "           4.7468e-02, -4.2799e-06],\n",
       "         [-1.7791e-01, -1.6620e-04, -1.1761e-05,  ..., -1.4434e-05,\n",
       "           2.0298e-05, -2.9571e-05]]),\n",
       " torch.Size([385, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_explainer.vector_representation, sm_explainer.vector_representation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss matrix aggregates the loss of applying each local model on every item in the training set. For example, the first row is the loss of using the local model of the first item to predict the whole training set. The average loss is 0.1743."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.2254e-06, 7.5176e-02, 1.1192e-04,  ..., 8.8568e-02, 6.6603e-07,\n",
       "          4.6529e-01],\n",
       "         [1.3241e-01, 7.5693e-04, 1.9821e-01,  ..., 3.9171e-03, 6.8754e-01,\n",
       "          3.0139e-02],\n",
       "         [3.6355e-06, 7.5328e-02, 1.1311e-04,  ..., 8.8746e-02, 2.0069e-07,\n",
       "          4.6494e-01],\n",
       "         ...,\n",
       "         [1.1518e-01, 2.9021e-02, 1.2507e-01,  ..., 3.9614e-03, 4.4132e-01,\n",
       "          3.0452e-03],\n",
       "         [6.5626e-06, 7.5091e-02, 1.0721e-04,  ..., 8.8661e-02, 1.4181e-07,\n",
       "          4.6589e-01],\n",
       "         [1.1518e-01, 2.8824e-02, 1.2485e-01,  ..., 3.8463e-03, 4.4236e-01,\n",
       "          3.0747e-03]]),\n",
       " torch.Size([385, 385]),\n",
       " tensor(0.1743))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_explainer.get_L(), sm_explainer.get_L().shape, sm_explainer.get_L().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply reduction to get the proxies\n",
    "Here we use the greedy loss reduction method, with expected proxies number set to 5 and coverage expectation to 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explainreduce.proxies as px\n",
    "reduced_sm_explainer = px.find_proxies_greedy_k_min_loss(\n",
    "    explainer=sm_explainer,\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a reduced explanation set with 5 proxies, the average loss is 0.1795, very close to the original explanation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-8.8547e-06, -4.9765e-02, -1.8553e-02, -6.3732e-01,  1.9076e-06,\n",
       "           3.2872e-01, -1.5689e-01, -8.6860e-05,  1.6822e-02, -2.5269e-04],\n",
       "         [ 5.6257e-02,  9.3633e-02, -5.6361e-01, -6.3810e-01,  8.3662e-02,\n",
       "           3.6969e-01,  6.6815e-06, -4.5865e-03,  7.7091e-06, -9.9494e-02],\n",
       "         [-1.7224e-01, -1.1353e-01, -1.1800e-05, -2.6111e-01, -7.0723e-02,\n",
       "           1.9629e-01, -3.3596e-01,  1.4391e-05,  1.4776e-06, -1.3858e-04],\n",
       "         [-1.7675e-01,  5.7212e-06, -8.5173e-06, -3.5556e-01, -1.4104e-02,\n",
       "           1.5552e-01, -1.8900e-01, -4.9126e-06,  1.7652e-05, -4.5481e-03],\n",
       "         [ 1.0184e-05, -1.0307e-06, -4.0093e-01, -5.2470e-01,  8.5111e-06,\n",
       "           3.9684e-01, -6.0900e-02, -1.6036e-05,  2.3373e-02, -4.2997e-07]]),\n",
       " torch.Size([5, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_sm_explainer.vector_representation, reduced_sm_explainer.vector_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.4479e-02, 1.7692e-02, 2.4911e-02,  ..., 1.7085e-02, 6.7794e-02,\n",
       "          4.6061e-01],\n",
       "         [8.4502e-03, 1.4985e-01, 1.5279e-01,  ..., 1.9480e-01, 1.5140e-01,\n",
       "          9.6599e-01],\n",
       "         [1.3248e-01, 7.8132e-04, 1.9829e-01,  ..., 3.8762e-03, 6.8701e-01,\n",
       "          3.0390e-02],\n",
       "         [1.2086e-01, 3.6374e-02, 1.1913e-01,  ..., 4.9810e-03, 4.4925e-01,\n",
       "          1.4908e-03],\n",
       "         [3.3074e-03, 1.2810e-01, 1.7278e-02,  ..., 1.7730e-01, 3.8489e-02,\n",
       "          5.2490e-01]]),\n",
       " torch.Size([5, 385]),\n",
       " tensor(0.1795))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_sm_explainer.get_L(), reduced_sm_explainer.get_L().shape, reduced_sm_explainer.get_L().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the proxies to make predictions\n",
    "We have 7 test items, below we show the predictions on those items given by the black-box SVR, the full set of local explanations, and the proxies respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from the black box: [ 0.38712107 -0.12080159  1.43031837  0.03801568  0.64973534  0.75055089\n",
      " -1.35231908]\n",
      "Predictions from the full set: [ 0.3100698  -0.14047301  1.4497539  -0.15255666  0.53849864  0.6774812\n",
      " -1.4204621 ]\n",
      "Predictions from the proxies: [ 0.30051297 -0.02767373  1.4334548  -0.15312657  0.55653054  0.6969903\n",
      " -1.3894179 ]\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "print(f\"Predictions from the black box: {svr.predict(X_test_tensor)}\")\n",
    "print(f\"Predictions from the full set: {sm_explainer.predict(X_test_tensor).ravel().detach().numpy()}\")\n",
    "print(f\"Predictions from the proxies: {reduced_sm_explainer.predict(X_test_tensor).ravel().detach().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
